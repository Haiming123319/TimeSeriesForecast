#!/usr/bin/env python3
"""
ä¸‰é˜¶æ®µä¼˜åŒ–è®­ç»ƒç­–ç•¥
é˜¶æ®µ1: æ¨¡å‹ç­›é€‰ï¼ˆ2å·ï¼Œ30minï¼Œpred_len=24ï¼Œ4æ¨¡å‹ï¼‰
é˜¶æ®µ2: æ‰©å±•éªŒè¯ï¼ˆ5å·ï¼Œ30min+15minï¼Œpred_len=24ï¼Œå‰2æ¨¡å‹ï¼‰
é˜¶æ®µ3: æ­¥é•¿æ‰©å±•ï¼ˆæœ€ä¼˜æ¨¡å‹ï¼Œæ‰€æœ‰ç»„åˆï¼‰
"""

import os
import subprocess
import time
import json
import pandas as pd
import glob
import re
from pathlib import Path


# ============================================================
# è½»é‡çº§æ¨¡å‹é…ç½®ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
# ============================================================
MODELS_CONFIG = {
    'DLinear': {
        'params': '--e_layers 2 --d_layers 1',
        'batch_size': 64
    },
    'PatchTST': {
        'params': '--e_layers 2 --n_heads 4 --d_model 128 --d_ff 128 --patch_len 16',
        'batch_size': 32
    },
    'TimesNet': {
        # è½»é‡çº§é…ç½®ï¼šå‡å°‘e_layers, d_model, num_kernelsï¼Œé€‚åˆT4 GPU
        'params': '--e_layers 1 --d_model 128 --d_ff 128 --num_kernels 4 --top_k 3',
        'batch_size': 16
    },
    'iTransformer': {
        # è½»é‡çº§é…ç½®ï¼šå‡å°‘layerså’Œhidden size
        'params': '--e_layers 2 --d_model 128 --d_ff 256 --n_heads 4',
        'batch_size': 16
    }
}


def get_model_specific_params(model_name, device_type, seq_len, label_len, batch_size):
    """
    è·å–æ¨¡å‹ç‰¹å®šå‚æ•°ï¼Œé’ˆå¯¹ä¸åŒæ¨¡å‹å’Œè®¾å¤‡ä¼˜åŒ–
    
    ç‰¹æ®Šå¤„ç†ï¼š
    - TimesNet: å…³é—­AMPï¼ˆé¿å…cuFFT FP16é”™è¯¯ï¼‰ï¼Œä½¿ç”¨è½»é‡é…ç½®
    - å…¶ä»–æ¨¡å‹: å¼€å¯AMPåŠ é€Ÿ
    """
    name = model_name.lower()
    
    # åŸºç¡€å‚æ•°
    num_workers = 2 if device_type == 'cuda' else (4 if device_type == 'mps' else 2)
    
    params = {
        'num_workers': num_workers,
        'seq_len': seq_len,
        'label_len': label_len,
        'batch_size': batch_size,
        'use_amp': True,  # é»˜è®¤å¼€å¯AMP
    }
    
    # TimesNetç‰¹æ®Šå¤„ç†ï¼šå…³é—­AMPé¿å…cuFFT FP16é”™è¯¯
    if name == 'timesnet':
        params['use_amp'] = False  # å…³é”®ï¼šTimesNetä¸ç”¨AMP
        params['batch_size'] = 16  # è½»é‡é…ç½®
        print(f"  âš ï¸  TimesNetå…³é—­AMP (é¿å…cuFFT FP16é”™è¯¯)ï¼Œä½¿ç”¨FP32è®­ç»ƒ")
    
    return params


def run_experiment(data_path, model_name, model_id, seq_len, label_len, pred_len, 
                   batch_size, epochs=10, patience=2, use_gpu=True):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    
    config = MODELS_CONFIG[model_name]
    
    # æ£€æµ‹è®¾å¤‡ç±»å‹
    import torch
    device_type = 'cuda'
    if use_gpu:
        if torch.cuda.is_available():
            device_type = 'cuda'
        elif torch.backends.mps.is_available():
            device_type = 'mps'
        else:
            use_gpu = False
    
    # è·å–æ¨¡å‹ç‰¹å®šå‚æ•°ï¼ˆç‰¹åˆ«å¤„ç†TimesNetï¼‰
    model_params = get_model_specific_params(model_name, device_type, seq_len, label_len, batch_size)
    
    # æ„å»ºAMPå‚æ•°ï¼ˆTimesNetä¸ç”¨ï¼Œå…¶ä»–æ¨¡å‹ç”¨ï¼‰
    amp_flag = '--use_amp' if model_params['use_amp'] else ''
    
    # æ„å»ºGPUå‚æ•°
    if device_type == 'mps':
        gpu_params = '--use_gpu 1 --gpu_type mps'
    else:
        gpu_params = f"--use_gpu {1 if use_gpu else 0} --gpu 0"
    
    cmd = f"""python3 run.py \
      --task_name long_term_forecast \
      --is_training 1 \
      --root_path ./data/AEMO_optimized/ \
      --data_path {data_path} \
      --model_id {model_id} \
      --model {model_name} \
      --data custom \
      --features M \
      --target Price \
      --seq_len {model_params['seq_len']} \
      --label_len {model_params['label_len']} \
      --pred_len {pred_len} \
      --enc_in 5 \
      --dec_in 5 \
      --c_out 5 \
      --train_epochs {epochs} \
      --batch_size {model_params['batch_size']} \
      --patience {patience} \
      --learning_rate 0.001 \
      --num_workers {model_params['num_workers']} \
      {gpu_params} \
      {amp_flag} \
      {config['params']}"""
    
    start_time = time.time()
    result = subprocess.run(cmd, shell=True, capture_output=False)
    elapsed = time.time() - start_time
    
    return result.returncode == 0, elapsed


def extract_metrics(model_id):
    """ä»ç»“æœæ–‡ä»¶ä¸­æå–æŒ‡æ ‡"""
    result_files = glob.glob(f'./results/*{model_id}*/result*.txt')
    
    if not result_files:
        return None
    
    try:
        with open(result_files[0], 'r') as f:
            content = f.read()
        
        mse_match = re.search(r'mse:([\d.]+)', content)
        mae_match = re.search(r'mae:([\d.]+)', content)
        
        if mse_match and mae_match:
            return {
                'mse': float(mse_match.group(1)),
                'mae': float(mae_match.group(1))
            }
    except:
        pass
    
    return None


# ============================================================
# é˜¶æ®µ1: æ¨¡å‹ç­›é€‰
# ============================================================
def stage1_model_selection(use_gpu=True):
    """
    é˜¶æ®µ1: æ¨¡å‹ç­›é€‰
    - 2ä¸ªä»£è¡¨æ€§å· (NSW=ä½“é‡å¤§, SA=æ³¢åŠ¨å¤§)
    - åªç”¨30åˆ†é’Ÿæ•°æ®
    - pred_len=24
    - 4ä¸ªæ¨¡å‹å¿«é€Ÿå¯¹æ¯”
    """
    
    print("\n" + "="*80)
    print("ğŸ¯ é˜¶æ®µ1: æ¨¡å‹ç­›é€‰ï¼ˆå¿«é€Ÿå¯¹æ¯”4ä¸ªæ¨¡å‹ï¼‰")
    print("="*80)
    print("é…ç½®:")
    print("  - å·: NSW (ä½“é‡å¤§), SA (æ³¢åŠ¨å¤§)")
    print("  - é¢‘ç‡: 30min")
    print("  - é¢„æµ‹æ­¥é•¿: 24 (12å°æ—¶)")
    print("  - è®­ç»ƒè½®æ•°: 8 epochs, æ—©åœpatience=2")
    print("  - ç›®æ ‡: é€‰å‡ºå‰2ä¸ªæœ€ä¼˜æ¨¡å‹")
    print("="*80)
    print()
    
    states = ['NSW', 'SA']
    models = ['DLinear', 'PatchTST', 'TimesNet', 'iTransformer']
    pred_len = 24
    seq_len = 192  # 4å¤©å†å²ï¼ˆ30miné—´éš”ï¼‰
    label_len = 96
    
    results = []
    total = len(states) * len(models)
    count = 0
    
    for state in states:
        for model in models:
            count += 1
            
            data_file = f'{state}_30min.csv'
            model_id = f'stage1_{state}_{model}'
            
            print(f"[{count}/{total}] ğŸš€ {state} | {model}")
            print("-"*80)
            
            success, elapsed = run_experiment(
                data_path=data_file,
                model_name=model,
                model_id=model_id,
                seq_len=seq_len,
                label_len=label_len,
                pred_len=pred_len,
                batch_size=MODELS_CONFIG[model]['batch_size'],
                epochs=8,
                patience=2,
                use_gpu=use_gpu
            )
            
            metrics = extract_metrics(model_id) if success else None
            
            result = {
                'stage': 1,
                'state': state,
                'freq': '30min',
                'model': model,
                'pred_len': pred_len,
                'success': success,
                'time_minutes': elapsed / 60,
                'mse': metrics['mse'] if metrics else None,
                'mae': metrics['mae'] if metrics else None
            }
            
            results.append(result)
            
            status = "âœ…" if success else "âŒ"
            if metrics:
                print(f"{status} å®Œæˆ | MAE: {metrics['mae']:.4f} | ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ\n")
            else:
                print(f"{status} å®Œæˆ | ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ\n")
    
    # åˆ†æç»“æœï¼Œé€‰å‡ºå‰2ä¸ªæ¨¡å‹
    df = pd.DataFrame(results)
    df_success = df[df['success'] == True].copy()
    
    if len(df_success) > 0 and df_success['mae'].notna().any():
        # æŒ‰MAEæ’åº
        df_success = df_success.sort_values('mae')
        
        print("\n" + "="*80)
        print("ğŸ“Š é˜¶æ®µ1ç»“æœæ±‡æ€»")
        print("="*80)
        print(df_success[['state', 'model', 'mae', 'mse', 'time_minutes']].to_string(index=False))
        
        # é€‰å‡ºå‰2ä¸ªæ¨¡å‹ï¼ˆæŒ‰å¹³å‡MAEï¼‰
        model_avg = df_success.groupby('model')['mae'].mean().sort_values()
        top2_models = model_avg.head(2).index.tolist()
        
        print(f"\nâœ¨ é€‰ä¸­çš„å‰2ä¸ªæ¨¡å‹:")
        for i, model in enumerate(top2_models, 1):
            avg_mae = model_avg[model]
            print(f"  {i}. {model:15s} - å¹³å‡MAE: {avg_mae:.4f}")
        
        return top2_models, results
    else:
        print("\nâš ï¸  é˜¶æ®µ1æ²¡æœ‰æˆåŠŸçš„å®éªŒ")
        return [], results


# ============================================================
# é˜¶æ®µ2: æ‰©å±•éªŒè¯
# ============================================================
def stage2_expansion(top2_models, use_gpu=True):
    """
    é˜¶æ®µ2: æ‰©å±•éªŒè¯
    - æ‰€æœ‰5ä¸ªå·
    - 30min + 15min ä¸¤ä¸ªé¢‘ç‡
    - pred_len=24
    - åªç”¨é˜¶æ®µ1é€‰å‡ºçš„å‰2ä¸ªæ¨¡å‹
    """
    
    if not top2_models:
        print("\nâš ï¸  é˜¶æ®µ1æœªé€‰å‡ºæ¨¡å‹ï¼Œè·³è¿‡é˜¶æ®µ2")
        return []
    
    print("\n" + "="*80)
    print("ğŸ¯ é˜¶æ®µ2: æ‰©å±•éªŒè¯ï¼ˆæ‰€æœ‰å·+ä¸¤ä¸ªé¢‘ç‡ï¼‰")
    print("="*80)
    print("é…ç½®:")
    print(f"  - æ¨¡å‹: {', '.join(top2_models)}")
    print("  - å·: å…¨éƒ¨5ä¸ª (NSW, QLD, VIC, SA, TAS)")
    print("  - é¢‘ç‡: 30min, 15min")
    print("  - é¢„æµ‹æ­¥é•¿: 24")
    print("  - è®­ç»ƒè½®æ•°: 10 epochs, æ—©åœpatience=2")
    print("="*80)
    print()
    
    states = ['NSW', 'QLD', 'VIC', 'SA', 'TAS']
    freqs = [
        ('30min', 192, 96),  # (freq, seq_len, label_len)
        ('15min', 384, 192)
    ]
    pred_len = 24
    
    results = []
    total = len(states) * len(freqs) * len(top2_models)
    count = 0
    
    for state in states:
        for freq, seq_len, label_len in freqs:
            for model in top2_models:
                count += 1
                
                data_file = f'{state}_{freq}.csv'
                model_id = f'stage2_{state}_{freq}_{model}'
                
                print(f"[{count}/{total}] ğŸš€ {state} | {freq} | {model}")
                print("-"*80)
                
                success, elapsed = run_experiment(
                    data_path=data_file,
                    model_name=model,
                    model_id=model_id,
                    seq_len=seq_len,
                    label_len=label_len,
                    pred_len=pred_len,
                    batch_size=MODELS_CONFIG[model]['batch_size'],
                    epochs=10,
                    patience=2,
                    use_gpu=use_gpu
                )
                
                metrics = extract_metrics(model_id) if success else None
                
                result = {
                    'stage': 2,
                    'state': state,
                    'freq': freq,
                    'model': model,
                    'pred_len': pred_len,
                    'success': success,
                    'time_minutes': elapsed / 60,
                    'mse': metrics['mse'] if metrics else None,
                    'mae': metrics['mae'] if metrics else None
                }
                
                results.append(result)
                
                status = "âœ…" if success else "âŒ"
                if metrics:
                    print(f"{status} å®Œæˆ | MAE: {metrics['mae']:.4f} | ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ\n")
                else:
                    print(f"{status} å®Œæˆ | ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ\n")
    
    # åˆ†æç»“æœ
    df = pd.DataFrame(results)
    df_success = df[df['success'] == True].copy()
    
    if len(df_success) > 0 and df_success['mae'].notna().any():
        print("\n" + "="*80)
        print("ğŸ“Š é˜¶æ®µ2ç»“æœæ±‡æ€»")
        print("="*80)
        
        # æŒ‰é¢‘ç‡åˆ†ç»„æ˜¾ç¤º
        for freq in ['30min', '15min']:
            df_freq = df_success[df_success['freq'] == freq]
            if len(df_freq) > 0:
                print(f"\n{freq} æ•°æ®:")
                print(df_freq[['state', 'model', 'mae', 'mse']].sort_values('mae').to_string(index=False))
        
        # é€‰å‡ºæœ€ä¼˜æ¨¡å‹
        model_avg = df_success.groupby('model')['mae'].mean().sort_values()
        best_model = model_avg.index[0]
        
        print(f"\nâœ¨ æœ€ä¼˜æ¨¡å‹: {best_model}")
        print(f"   å¹³å‡MAE: {model_avg[best_model]:.4f}")
        
        return best_model, results
    else:
        print("\nâš ï¸  é˜¶æ®µ2æ²¡æœ‰æˆåŠŸçš„å®éªŒ")
        return None, results


# ============================================================
# é˜¶æ®µ3: æ­¥é•¿æ‰©å±•
# ============================================================
def stage3_pred_len_expansion(best_model, use_gpu=True):
    """
    é˜¶æ®µ3: æ­¥é•¿æ‰©å±•
    - åªç”¨æœ€ä¼˜æ¨¡å‹
    - æ‰€æœ‰å·
    - 30miné¢‘ç‡ï¼ˆä¸»è¦ï¼‰
    - pred_len = 24, 48, 96
    """
    
    if not best_model:
        print("\nâš ï¸  æœªæ‰¾åˆ°æœ€ä¼˜æ¨¡å‹ï¼Œè·³è¿‡é˜¶æ®µ3")
        return []
    
    print("\n" + "="*80)
    print("ğŸ¯ é˜¶æ®µ3: é¢„æµ‹æ­¥é•¿æ‰©å±•")
    print("="*80)
    print("é…ç½®:")
    print(f"  - æ¨¡å‹: {best_model}")
    print("  - å·: å…¨éƒ¨5ä¸ª")
    print("  - é¢‘ç‡: 30min")
    print("  - é¢„æµ‹æ­¥é•¿: 24, 48, 96")
    print("  - è®­ç»ƒè½®æ•°: 15 epochs, æ—©åœpatience=3")
    print("="*80)
    print()
    
    states = ['NSW', 'QLD', 'VIC', 'SA', 'TAS']
    pred_lens = [24, 48, 96]
    seq_len = 192
    label_len = 96
    
    results = []
    total = len(states) * len(pred_lens)
    count = 0
    
    for state in states:
        for pred_len in pred_lens:
            count += 1
            
            data_file = f'{state}_30min.csv'
            model_id = f'stage3_{state}_{best_model}_pl{pred_len}'
            
            print(f"[{count}/{total}] ğŸš€ {state} | pred_len={pred_len}")
            print("-"*80)
            
            success, elapsed = run_experiment(
                data_path=data_file,
                model_name=best_model,
                model_id=model_id,
                seq_len=seq_len,
                label_len=label_len,
                pred_len=pred_len,
                batch_size=MODELS_CONFIG[best_model]['batch_size'],
                epochs=15,
                patience=3,
                use_gpu=use_gpu
            )
            
            metrics = extract_metrics(model_id) if success else None
            
            result = {
                'stage': 3,
                'state': state,
                'freq': '30min',
                'model': best_model,
                'pred_len': pred_len,
                'success': success,
                'time_minutes': elapsed / 60,
                'mse': metrics['mse'] if metrics else None,
                'mae': metrics['mae'] if metrics else None
            }
            
            results.append(result)
            
            status = "âœ…" if success else "âŒ"
            if metrics:
                print(f"{status} å®Œæˆ | MAE: {metrics['mae']:.4f} | ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ\n")
            else:
                print(f"{status} å®Œæˆ | ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ\n")
    
    # åˆ†æç»“æœ
    df = pd.DataFrame(results)
    df_success = df[df['success'] == True].copy()
    
    if len(df_success) > 0 and df_success['mae'].notna().any():
        print("\n" + "="*80)
        print("ğŸ“Š é˜¶æ®µ3ç»“æœæ±‡æ€»")
        print("="*80)
        
        # æŒ‰é¢„æµ‹æ­¥é•¿åˆ†ç»„
        for pred_len in pred_lens:
            df_pred = df_success[df_success['pred_len'] == pred_len]
            if len(df_pred) > 0:
                print(f"\né¢„æµ‹{pred_len}æ­¥ ({'12å°æ—¶' if pred_len==24 else '24å°æ—¶' if pred_len==48 else '48å°æ—¶'}):")
                print(df_pred[['state', 'mae', 'mse']].sort_values('mae').to_string(index=False))
        
        return results
    else:
        print("\nâš ï¸  é˜¶æ®µ3æ²¡æœ‰æˆåŠŸçš„å®éªŒ")
        return results


# ============================================================
# ä¸»æµç¨‹
# ============================================================
def main():
    """ä¸»æµç¨‹ï¼šæ‰§è¡Œä¸‰é˜¶æ®µè®­ç»ƒ"""
    
    print("="*80)
    print("ğŸš€ AEMOæ—¶åºé¢„æµ‹ - ä¸‰é˜¶æ®µä¼˜åŒ–è®­ç»ƒ")
    print("="*80)
    print("ç­–ç•¥è¯´æ˜:")
    print("  é˜¶æ®µ1: æ¨¡å‹ç­›é€‰ï¼ˆ2å· Ã— 4æ¨¡å‹ = 8ä¸ªå®éªŒï¼Œçº¦1-2å°æ—¶ï¼‰")
    print("  é˜¶æ®µ2: æ‰©å±•éªŒè¯ï¼ˆ5å· Ã— 2é¢‘ç‡ Ã— 2æ¨¡å‹ = 20ä¸ªå®éªŒï¼Œçº¦3-4å°æ—¶ï¼‰")
    print("  é˜¶æ®µ3: æ­¥é•¿æ‰©å±•ï¼ˆ5å· Ã— 3æ­¥é•¿ Ã— 1æ¨¡å‹ = 15ä¸ªå®éªŒï¼Œçº¦2-3å°æ—¶ï¼‰")
    print("  æ€»è®¡: çº¦43ä¸ªå®éªŒï¼Œç›¸æ¯”åŸæ–¹æ¡ˆï¼ˆ120ä¸ªï¼‰å‡å°‘64%")
    print("="*80)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists('./data/AEMO_optimized'):
        print("\nâŒ é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        print("è¯·å…ˆè¿è¡Œ: python3 generate_optimized_aemo_data.py")
        return
    
    # æ£€æµ‹è®¾å¤‡å¹¶ä¼˜åŒ–
    import torch
    
    # å¯ç”¨cuDNN benchmarkä¼˜åŒ–ï¼ˆå¯¹TimesNetç‰¹åˆ«æœ‰æ•ˆï¼‰
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        print("âœ… cuDNN benchmark å·²å¯ç”¨")
    
    use_gpu = torch.cuda.is_available() or torch.backends.mps.is_available()
    
    # æ‰“å°è¯¦ç»†è®¾å¤‡ä¿¡æ¯
    if torch.cuda.is_available():
        device_name = f"GPU (CUDA: {torch.cuda.get_device_name(0)})"
        print(f"\nğŸ–¥ï¸  è®¾å¤‡: {device_name}")
        print(f"ğŸ’¾ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"âš¡ AMP: å·²å¯ç”¨ (FP16æ··åˆç²¾åº¦)")
        print(f"ğŸ‘· Workers: 2 (ä¼˜åŒ–Colabæ€§èƒ½)")
    elif torch.backends.mps.is_available():
        device_name = "MPS (Apple Silicon)"
        print(f"\nğŸ–¥ï¸  è®¾å¤‡: {device_name}")
        print(f"âš¡ AMP: å·²å¯ç”¨")
        print(f"ğŸ‘· Workers: 4")
    else:
        device_name = "CPU"
        print(f"\nğŸ–¥ï¸  è®¾å¤‡: {device_name}")
        print(f"ğŸ‘· Workers: 2")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs('./three_stage_results', exist_ok=True)
    
    start_time = time.time()
    all_results = []
    
    # é˜¶æ®µ1: æ¨¡å‹ç­›é€‰
    top2_models, stage1_results = stage1_model_selection(use_gpu=use_gpu)
    all_results.extend(stage1_results)
    
    # ä¿å­˜é˜¶æ®µ1ç»“æœ
    with open('./three_stage_results/stage1_results.json', 'w') as f:
        json.dump(stage1_results, f, indent=2)
    
    if not top2_models:
        print("\nâŒ é˜¶æ®µ1å¤±è´¥ï¼Œåœæ­¢è®­ç»ƒ")
        return
    
    # é˜¶æ®µ2: æ‰©å±•éªŒè¯
    best_model, stage2_results = stage2_expansion(top2_models, use_gpu=use_gpu)
    all_results.extend(stage2_results)
    
    # ä¿å­˜é˜¶æ®µ2ç»“æœ
    with open('./three_stage_results/stage2_results.json', 'w') as f:
        json.dump(stage2_results, f, indent=2)
    
    if not best_model:
        print("\nâŒ é˜¶æ®µ2å¤±è´¥ï¼Œåœæ­¢è®­ç»ƒ")
        return
    
    # é˜¶æ®µ3: æ­¥é•¿æ‰©å±•
    stage3_results = stage3_pred_len_expansion(best_model, use_gpu=use_gpu)
    all_results.extend(stage3_results)
    
    # ä¿å­˜é˜¶æ®µ3ç»“æœ
    with open('./three_stage_results/stage3_results.json', 'w') as f:
        json.dump(stage3_results, f, indent=2)
    
    # æ€»ç»“
    elapsed = time.time() - start_time
    hours = int(elapsed // 3600)
    minutes = int((elapsed % 3600) // 60)
    
    print("\n" + "="*80)
    print("ğŸ‰ ä¸‰é˜¶æ®µè®­ç»ƒå®Œæˆï¼")
    print("="*80)
    print(f"â±ï¸  æ€»è€—æ—¶: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ")
    print(f"âœ… æ€»å®éªŒæ•°: {len(all_results)}")
    print(f"âœ… æˆåŠŸ: {len([r for r in all_results if r['success']])}")
    print(f"âŒ å¤±è´¥: {len([r for r in all_results if not r['success']])}")
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    df_all = pd.DataFrame(all_results)
    df_all.to_csv('./three_stage_results/all_results.csv', index=False)
    
    print("\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: ./three_stage_results/")
    print("  - stage1_results.json")
    print("  - stage2_results.json")
    print("  - stage3_results.json")
    print("  - all_results.csv")


if __name__ == '__main__':
    main()

